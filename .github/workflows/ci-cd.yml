name: CI/CD Pipeline

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master ]

env:
  IMAGE_NAME: zoneapi
  HELM_VERSION: 3.12.0

permissions:
  contents: read
  security-events: write
  actions: read
  id-token: write
  pull-requests: read

jobs:
  # Build and Test Job
  build-and-test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: 7.0.x
    
    - name: Restore dependencies
      run: dotnet restore ZoneAPI/ZoneAPI.csproj
    
    - name: Build application
      run: dotnet build ZoneAPI/ZoneAPI.csproj --no-restore --configuration Release
    
    - name: Run tests
      run: dotnet test ZoneAPI/ZoneAPI.csproj --no-build --configuration Release --verbosity normal
    
    # Upload build artifacts
    - name: Upload build artifacts
      uses: actions/upload-artifact@v4
      with:
        name: build-artifacts
        path: ZoneAPI/bin/Release/
        retention-days: 1

  # Security Scanning Job
  security-scan:
    runs-on: ubuntu-latest
    needs: build-and-test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: üîí Run Trivy Vulnerability Scanner on Source Code
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-source-results.sarif'
        
    - name: üìä Upload Source Code Scan Results to GitHub Security Tab
      if: always() && github.event_name != 'pull_request'
      uses: github/codeql-action/upload-sarif@v3
      with:
        sarif_file: 'trivy-source-results.sarif'
        category: 'trivy-source-code'
        
    - name: üîç Run Trivy Source Code Table Report
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'table'
        ignore-unfixed: true
        severity: 'CRITICAL,HIGH,MEDIUM'
        
    - name: üìã Run Secret Detection Scan
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        scanners: 'secret'
        format: 'table'
        exit-code: '1'
        
    - name: üìä Upload SARIF for Pull Requests (Alternative)
      if: github.event_name == 'pull_request'
      uses: actions/upload-artifact@v4
      with:
        name: trivy-source-sarif-pr
        path: trivy-source-results.sarif
        retention-days: 7

    - name: üì§ Upload Source Security Report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: trivy-source-security-report
        path: trivy-source-results.sarif
        retention-days: 30

  # Infrastructure Deployment Job
  deploy-infrastructure:
    runs-on: ubuntu-latest
    needs: [build-and-test, security-scan]
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
    
    outputs:
      aks-cluster-name: ${{ steps.terraform-output.outputs.aks-cluster-name }}
      resource-group: ${{ steps.terraform-output.outputs.resource-group }}
      postgres-host: ${{ steps.terraform-output.outputs.postgres-host }}
      acr-login-server: ${{ steps.terraform-output.outputs.acr-login-server }}
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.5.0
        terraform_wrapper: false
    
    - name: Terraform Format Check
      run: terraform fmt -check
      working-directory: ./terraform
    
    - name: Setup Terraform Backend (if needed)
      run: |
        # Check if backend storage account exists, create if not
        if ! az storage account show --name tfstatezoneapi --resource-group rg-terraform-state >/dev/null 2>&1; then
          echo "Setting up Terraform backend..."
          ./scripts/setup-terraform-backend.sh
        else
          echo "Terraform backend already exists"
        fi
    
    - name: Terraform Init
      run: terraform init
      working-directory: ./terraform
      env:
        ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
        ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
        ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
        ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
    
    - name: Terraform Plan
      run: terraform plan -out=tfplan
      working-directory: ./terraform
      env:
        ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
        ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
        ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
        ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
        TF_VAR_postgres_admin_password: ${{ secrets.POSTGRES_ADMIN_PASSWORD }}
    
    - name: Terraform Apply
      run: terraform apply -auto-approve tfplan
      working-directory: ./terraform
      env:
        ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
        ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
        ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
        ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
        TF_VAR_postgres_admin_password: ${{ secrets.POSTGRES_ADMIN_PASSWORD }}
    
    - name: Get Terraform Outputs
      id: terraform-output
      run: |
        echo "aks-cluster-name=$(terraform output -raw aks_cluster_name)" >> $GITHUB_OUTPUT
        echo "resource-group=$(terraform output -raw resource_group_name)" >> $GITHUB_OUTPUT
        echo "postgres-host=$(terraform output -raw postgres_server_fqdn)" >> $GITHUB_OUTPUT
        echo "acr-login-server=$(terraform output -raw acr_login_server)" >> $GITHUB_OUTPUT
      working-directory: ./terraform
      env:
        ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
        ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
        ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
        ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}

  # Docker Build and Push Job
  docker-build-push:
    runs-on: ubuntu-latest
    needs: deploy-infrastructure
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
    
    outputs:
      image-tag: ${{ steps.image-tag.outputs.tag }}
      image-digest: ${{ steps.build.outputs.digest }}
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Generate image tag
      id: image-tag
      run: |
        if [ "${{ github.ref }}" == "refs/heads/main" ] || [ "${{ github.ref }}" == "refs/heads/master" ]; then
          echo "tag=latest" >> $GITHUB_OUTPUT
        else
          echo "tag=${{ github.sha }}" >> $GITHUB_OUTPUT
        fi
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.5.0
        terraform_wrapper: false
    
    - name: Get ACR login server from Terraform
      id: acr-info
      run: |
        # Setup Terraform backend (since this is a separate job)
        if ! az storage account show --name tfstatezoneapi --resource-group rg-terraform-state >/dev/null 2>&1; then
          echo "Setting up Terraform backend..."
          ./scripts/setup-terraform-backend.sh
        else
          echo "Terraform backend already exists"
        fi
        
        cd terraform
        # Initialize Terraform with backend
        terraform init
        ACR_LOGIN_SERVER=$(terraform output -raw acr_login_server)
        ACR_USERNAME=$(terraform output -raw acr_admin_username)
        ACR_PASSWORD=$(terraform output -raw acr_admin_password)
        echo "::add-mask::$ACR_PASSWORD"
        echo "login-server=$ACR_LOGIN_SERVER" >> $GITHUB_OUTPUT
        echo "username=$ACR_USERNAME" >> $GITHUB_OUTPUT
        echo "password=$ACR_PASSWORD" >> $GITHUB_OUTPUT
      env:
        ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
        ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
        ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
        ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
    
    - name: Log in to Azure Container Registry
      uses: azure/docker-login@v1
      with:
        login-server: ${{ steps.acr-info.outputs.login-server }}
        username: ${{ steps.acr-info.outputs.username }}
        password: ${{ steps.acr-info.outputs.password }}
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build and push Docker image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ steps.acr-info.outputs.login-server }}/${{ env.IMAGE_NAME }}:${{ steps.image-tag.outputs.tag }}
        labels: |
          org.opencontainers.image.title=${{ env.IMAGE_NAME }}
          org.opencontainers.image.description=ZoneAPI - Healthcare Appointment Management System
          org.opencontainers.image.source=${{ github.repository }}
          org.opencontainers.image.revision=${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

    - name: üîí Run Trivy Security Scan
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ steps.acr-info.outputs.login-server }}/${{ env.IMAGE_NAME }}:${{ steps.image-tag.outputs.tag }}
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: üìä Upload Trivy Scan Results to GitHub Security Tab
      if: always() && github.event_name != 'pull_request'
      uses: github/codeql-action/upload-sarif@v3
      with:
        sarif_file: 'trivy-results.sarif'
        category: 'trivy-container-image'

    - name: üîç Run Trivy Filesystem Scan
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'table'
        exit-code: '1'
        ignore-unfixed: true
        severity: 'CRITICAL,HIGH'

    - name: üìã Generate Trivy Security Report
      if: always()
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ steps.acr-info.outputs.login-server }}/${{ env.IMAGE_NAME }}:${{ steps.image-tag.outputs.tag }}
        format: 'table'
        output: 'trivy-image-report.txt'

    - name: üì§ Upload Security Reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: trivy-security-reports
        path: |
          trivy-results.sarif
          trivy-image-report.txt
        retention-days: 30

# Migration Job (Industry Best Practice - Separate from Deployment)
  run-migration:
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure, docker-build-push]
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Get AKS credentials
      run: |
        az aks get-credentials \
          --resource-group ${{ needs.deploy-infrastructure.outputs.resource-group }} \
          --name ${{ needs.deploy-infrastructure.outputs.aks-cluster-name }} \
          --overwrite-existing

    - name: Setup Helm
      uses: azure/setup-helm@v3
      with:
        version: ${{ env.HELM_VERSION }}
    
    - name: üîê Create ACR Secret and Namespace Setup
      timeout-minutes: 2
      run: |
        echo "=== üîê CREATING NAMESPACE AND ACR SECRET ==="
        
        # Create namespace if it doesn't exist
        kubectl create namespace zoneapi --dry-run=client -o yaml | kubectl apply -f -
        
        # Get ACR credentials
        ACR_LOGIN_SERVER="${{ needs.deploy-infrastructure.outputs.acr-login-server }}"
        
        if [ -z "$ACR_LOGIN_SERVER" ]; then
          echo "‚ùå ACR login server not available"
          exit 1
        fi
        
        echo "ACR Login Server: $ACR_LOGIN_SERVER"
        
        # Get ACR credentials for secret creation
        ACR_USERNAME=$(az acr credential show --name "${ACR_LOGIN_SERVER%%.azurecr.io}" --query username --output tsv)
        ACR_PASSWORD=$(az acr credential show --name "${ACR_LOGIN_SERVER%%.azurecr.io}" --query passwords[0].value --output tsv)
        
        echo "Creating ACR secret in namespace..."
        kubectl create secret docker-registry acr-secret \
          --namespace=zoneapi \
          --docker-server="$ACR_LOGIN_SERVER" \
          --docker-username="$ACR_USERNAME" \
          --docker-password="$ACR_PASSWORD" \
          --dry-run=client -o yaml | kubectl apply -f -
        
        # Create database secret with correct password from Terraform
        echo "Creating database secret..."
        kubectl create secret generic zoneapi-db-secret \
          --from-literal=password="${{ secrets.POSTGRES_ADMIN_PASSWORD }}" \
          --namespace=zoneapi \
          --dry-run=client -o yaml | kubectl apply -f -
        
        # Verify secrets were created
        if kubectl get secret acr-secret -n zoneapi >/dev/null 2>&1; then
          echo "‚úÖ ACR secret created successfully"
        else
          echo "‚ùå Failed to create ACR secret"
          exit 1
        fi
        
        if kubectl get secret zoneapi-db-secret -n zoneapi >/dev/null 2>&1; then
          echo "‚úÖ Database secret created successfully"
        else
          echo "‚ùå Failed to create database secret"
          exit 1
        fi
        
        echo "‚úÖ Namespace and secrets setup completed successfully"
    
    - name: üßπ Clean Up Old Migration Jobs (Start with Clean Environment)
      timeout-minutes: 3
      run: |
        echo "=== üßπ CLEANING UP OLD MIGRATION JOBS ==="
        echo "Removing all previous migration jobs and pods for a clean start..."
        
        # Make cleanup script executable
        chmod +x ./scripts/cleanup-migration-jobs.sh
        
        # Run cleanup with force option to ensure everything is removed
        ./scripts/cleanup-migration-jobs.sh zoneapi true
        
        echo "‚úÖ Environment cleaned successfully!"

    - name: üîß Validate Environment Variables
      timeout-minutes: 1
      run: |
        echo "=== üîß VALIDATING ENVIRONMENT VARIABLES ==="
        
        # Get environment variables
        ACR_LOGIN_SERVER="${{ needs.deploy-infrastructure.outputs.acr-login-server }}"
        DATABASE_HOST="${{ needs.deploy-infrastructure.outputs.postgres-host }}"
        DB_PASSWORD="${{ secrets.POSTGRES_ADMIN_PASSWORD }}"
        IMAGE_TAG="${{ needs.docker-build-push.outputs.image-tag }}"
        NAMESPACE="zoneapi"
        
        # Validate environment variables
        echo "Required Environment Variables:"
        echo "ACR_LOGIN_SERVER: ${ACR_LOGIN_SERVER:-[NOT SET]}"
        echo "DATABASE_HOST: ${DATABASE_HOST:-[NOT SET]}"
        echo "DB_PASSWORD: ${DB_PASSWORD:+[PROVIDED]}"
        echo "IMAGE_TAG: ${IMAGE_TAG:-[NOT SET]}"
        echo "NAMESPACE: ${NAMESPACE:-[NOT SET]}"
        
        echo ""
        echo "Constructed Image Path:"
        if [ -n "$ACR_LOGIN_SERVER" ] && [ -n "$IMAGE_TAG" ]; then
            echo "Full Image: ${ACR_LOGIN_SERVER}/zoneapi:${IMAGE_TAG}"
        else
            echo "‚ùå Cannot construct image path - missing ACR_LOGIN_SERVER or IMAGE_TAG"
            exit 1
        fi
        
        echo ""
        echo "Database Connection String:"
        if [ -n "$DATABASE_HOST" ] && [ -n "$DB_PASSWORD" ]; then
            echo "Connection: Host=${DATABASE_HOST};Port=5432;Database=zone;Username=postgres;Password=[HIDDEN];CommandTimeout=300;Timeout=60;"
        else
            echo "‚ùå Cannot construct connection string - missing DATABASE_HOST or DB_PASSWORD"
            exit 1
        fi
        
        echo "‚úÖ Environment variables validated!"

    - name: üß™ Test Database Connection from Kubernetes (Fixed Environment Variables)
      timeout-minutes: 2
      run: |
        echo "=== üß™ TESTING DATABASE CONNECTION FROM KUBERNETES ==="
        
        # Get environment variables
        DB_HOST="${{ needs.deploy-infrastructure.outputs.postgres-host }}"
        DB_PASSWORD="${{ secrets.POSTGRES_ADMIN_PASSWORD }}"
        
        # Debug: Show what values we have
        echo "Debug - DB_HOST: ${DB_HOST:-[EMPTY]}"
        echo "Debug - DB_PASSWORD: ${DB_PASSWORD:+[PROVIDED]}"
        
        if [ -z "$DB_HOST" ] || [ -z "$DB_PASSWORD" ]; then
          echo "‚ùå Missing database connection parameters"
          exit 1
        fi
        
        echo "Creating database connection test pod with properly substituted variables..."
        
        # Clean up any existing test pod first
        kubectl delete pod zoneapi-test-connection -n zoneapi --ignore-not-found=true
        
        # Create pod manifest with variables properly substituted
        cat > /tmp/db-test-pod.yaml << EOF
        apiVersion: v1
        kind: Pod
        metadata:
          name: zoneapi-test-connection
          namespace: zoneapi
        spec:
          restartPolicy: Never
          containers:
          - name: test-connection
            image: postgres:15-alpine
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: zoneapi-db-secret
                  key: password
            - name: DB_HOST
              value: "$DB_HOST"
            command:
            - /bin/sh
            - -c
            - |
              echo "Starting database connection test..."
              echo "Host: \$DB_HOST"
              echo ""
              echo "Step 1: Testing DNS resolution..."
              nslookup "\$DB_HOST" || exit 1
              echo "‚úÖ DNS resolution successful"
              
              echo ""
              echo "Step 2: Testing PostgreSQL connectivity..."
              pg_isready -h "\$DB_HOST" -p 5432 -U postgres || exit 1
              echo "‚úÖ PostgreSQL service is ready"
              
              echo ""
              echo "Step 3: Testing authentication..."
              psql -h "\$DB_HOST" -U postgres -d postgres -c "SELECT version();" || exit 1
              echo "‚úÖ Authentication successful"
              
              echo ""
              echo "Step 4: Testing database access..."
              if ! psql -h "\$DB_HOST" -U postgres -d zone -c "SELECT current_database();" 2>/dev/null; then
                echo "Database 'zone' doesn't exist, creating it..."
                psql -h "\$DB_HOST" -U postgres -d postgres -c "CREATE DATABASE zone;" || exit 1
                echo "‚úÖ Database 'zone' created"
              fi
              psql -h "\$DB_HOST" -U postgres -d zone -c "SELECT current_database();"
              echo "‚úÖ Target database 'zone' accessible"
              
              echo ""
              echo "Step 5: Testing connection string format..."
              echo "Connection string format: Host=\$DB_HOST;Port=5432;Database=zone;Username=postgres;***;CommandTimeout=300;"
              
              echo ""
              echo "üéâ All connection tests passed!"
              echo "Database is ready for migrations."
              echo "Keeping pod alive for 60 seconds for log inspection..."
              sleep 60
        EOF
        
        # Apply the pod manifest
        kubectl apply -f /tmp/db-test-pod.yaml
        
        echo "Waiting for test pod to complete..."
        kubectl wait --for=condition=ready pod/zoneapi-test-connection -n zoneapi --timeout=60s || true
        
        # Wait for pod to complete or timeout
        timeout=120
        while [ $timeout -gt 0 ]; do
          status=$(kubectl get pod zoneapi-test-connection -n zoneapi -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
          if [ "$status" = "Succeeded" ]; then
            echo "‚úÖ Database connection test completed successfully"
            kubectl logs zoneapi-test-connection -n zoneapi
            break
          elif [ "$status" = "Failed" ]; then
            echo "‚ùå Database connection test failed"
            kubectl logs zoneapi-test-connection -n zoneapi
            kubectl describe pod zoneapi-test-connection -n zoneapi
            exit 1
          fi
          echo "Waiting for test to complete... (status: $status, timeout: ${timeout}s)"
          sleep 5
          timeout=$((timeout - 5))
        done
        
        if [ $timeout -le 0 ]; then
          echo "‚ùå Database connection test timed out"
          kubectl logs zoneapi-test-connection -n zoneapi || echo "No logs available"
          kubectl describe pod zoneapi-test-connection -n zoneapi
          exit 1
        fi
        
        # Clean up test pod
        kubectl delete pod zoneapi-test-connection -n zoneapi --ignore-not-found=true

    - name: üìä Pre-Migration Database State Check (Fixed Environment Variables)
      timeout-minutes: 3
      run: |
        echo "=== üìä PRE-MIGRATION DATABASE STATE CHECK ==="
        
        DB_HOST="${{ needs.deploy-infrastructure.outputs.postgres-host }}"
        
        if [ -z "$DB_HOST" ]; then
          echo "‚ùå Database host not available"
          exit 1
        fi
        
        echo "Database Host: $DB_HOST"
        echo "Creating database state check pod..."
        
        # Create pod manifest with variables properly substituted
        cat > /tmp/db-state-check.yaml << EOF
        apiVersion: v1
        kind: Pod
        metadata:
          name: db-state-check
          namespace: zoneapi
        spec:
          restartPolicy: Never
          containers:
          - name: db-state
            image: postgres:15-alpine
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: zoneapi-db-secret
                  key: password
            - name: DB_HOST
              value: "$DB_HOST"
            command:
            - /bin/sh
            - -c
            - |
              echo "Checking database state on host: \$DB_HOST"
              echo ""
              echo "Tables in database:"
              psql -h "\$DB_HOST" -U postgres -d zone -c "\\dt" 2>/dev/null || echo "No tables found (database may be empty)"
              
              echo ""
              echo "Schema information:"
              psql -h "\$DB_HOST" -U postgres -d zone -c "SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';" 2>/dev/null || echo "No schema information available"
              
              echo ""
              echo "Migration history (if exists):"
              if psql -h "\$DB_HOST" -U postgres -d zone -c "SELECT COUNT(*) as migration_history FROM __EFMigrationsHistory;" 2>/dev/null; then
                echo "Migration history table exists"
                psql -h "\$DB_HOST" -U postgres -d zone -c "SELECT migration_id, product_version FROM __EFMigrationsHistory ORDER BY migration_id;" 2>/dev/null || true
              else
                echo "Migration history table does not exist yet (first run)"
              fi
              
              echo ""
              echo "‚úÖ Database state check completed"
        EOF
        
        # Apply the pod manifest
        kubectl apply -f /tmp/db-state-check.yaml
        
        echo "Waiting for database state check to complete..."
        
        # Wait for pod to complete
        timeout=120
        while [ $timeout -gt 0 ]; do
          status=$(kubectl get pod db-state-check -n zoneapi -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
          if [ "$status" = "Succeeded" ]; then
            echo "‚úÖ Database state check completed successfully"
            kubectl logs db-state-check -n zoneapi
            break
          elif [ "$status" = "Failed" ]; then
            echo "‚ö†Ô∏è Database state check failed (may be expected for first run)"
            kubectl logs db-state-check -n zoneapi || echo "No logs available"
            break
          fi
          echo "Waiting for state check to complete... (status: $status, timeout: ${timeout}s)"
          sleep 5
          timeout=$((timeout - 5))
        done
        
        # Clean up pod
        kubectl delete pod db-state-check -n zoneapi --ignore-not-found=true
        
        echo "‚úÖ Database state check completed!"

    - name: üöÄ Run Database Migration (efbundle Approach) with Enhanced Monitoring
      timeout-minutes: 2
      run: |
        echo "=== üöÄ RUNNING DATABASE MIGRATION (EFBUNDLE APPROACH) ==="
        
        # Set environment variables for migration script
        export ACR_LOGIN_SERVER="${{ needs.deploy-infrastructure.outputs.acr-login-server }}"
        export DATABASE_HOST="${{ needs.deploy-infrastructure.outputs.postgres-host }}"
        export DB_PASSWORD="${{ secrets.POSTGRES_ADMIN_PASSWORD }}"
        export IMAGE_TAG="${{ needs.docker-build-push.outputs.image-tag }}"
        export NAMESPACE="zoneapi"
        export TIMEOUT="180"  # 3 minute timeout - allows for image pull + migration execution
        
        # Debug: Check if environment variables are set
        echo "=== Environment Variable Debug ==="
        echo "ACR_LOGIN_SERVER: ${ACR_LOGIN_SERVER:-[EMPTY]}"
        echo "DATABASE_HOST: ${DATABASE_HOST:-[EMPTY]}"
        echo "IMAGE_TAG: ${IMAGE_TAG:-[EMPTY]}"
        
        # Final validation
        if [ -z "$ACR_LOGIN_SERVER" ] || [ -z "$DATABASE_HOST" ] || [ -z "$DB_PASSWORD" ] || [ -z "$IMAGE_TAG" ]; then
          echo "‚ùå Required environment variables are missing:"
          echo "ACR_LOGIN_SERVER: ${ACR_LOGIN_SERVER:-[MISSING]}"
          echo "DATABASE_HOST: ${DATABASE_HOST:-[MISSING]}"
          echo "DB_PASSWORD: ${DB_PASSWORD:+[PROVIDED]}"
          echo "IMAGE_TAG: ${IMAGE_TAG:-[MISSING]}"
          exit 1
        fi
        
        echo "Migration Configuration:"
        echo "- ACR: $ACR_LOGIN_SERVER"
        echo "- Database Host: $DATABASE_HOST"  
        echo "- Image Tag: $IMAGE_TAG"
        echo "- Namespace: $NAMESPACE"
        echo "- Timeout: $TIMEOUT seconds"
        
        # Make script executable
        chmod +x ./scripts/run-migration.sh
        
        # Start migration monitoring in background
        export CONTINUOUS_MONITORING=true
        ./scripts/monitor-pipeline.sh &
        MONITOR_PID=$!
        
        # Run migration using efbundle approach
        echo "üèÉ‚Äç‚ôÇÔ∏è Starting migration process..."
        if ./scripts/run-migration.sh; then
          echo "‚úÖ Migration script completed successfully!"
        else
          echo "‚ùå Migration script failed!"
          exit 1
        fi
        
        # Stop monitoring
        kill $MONITOR_PID 2>/dev/null || true
    
    - name: üîç Detailed Migration Job Analysis
      if: always()
      run: |
        echo "=== üîç DETAILED MIGRATION JOB ANALYSIS ==="
        
        echo "=== Migration Jobs ==="
        kubectl get jobs -n zoneapi -l app.kubernetes.io/component=migration -o wide
        
        echo "=== Migration Pods ==="
        kubectl get pods -n zoneapi -l app.kubernetes.io/component=migration -o wide
        
        echo "=== Recent Migration Events ==="
        kubectl get events --namespace=zoneapi --field-selector involvedObject.kind=Job --sort-by='.lastTimestamp' | tail -20
        
        # Get the latest migration job
        latest_job=$(kubectl get jobs -n zoneapi -l app.kubernetes.io/component=migration --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-1].metadata.name}' 2>/dev/null || echo "")
        
        if [ -n "$latest_job" ]; then
          echo "=== Latest Migration Job Details: $latest_job ==="
          kubectl describe job "$latest_job" -n zoneapi
          
          echo "=== Migration Job Logs ==="
          kubectl logs -l job-name="$latest_job" -n zoneapi --tail=200 || echo "No logs available"
          
          # Check job status
          job_status=$(kubectl get job "$latest_job" -n zoneapi -o jsonpath='{.status.conditions[0].type}' 2>/dev/null || echo "Unknown")
          echo "Migration Job Status: $job_status"
          
          if [ "$job_status" = "Complete" ]; then
            echo "‚úÖ Migration completed successfully!"
          elif [ "$job_status" = "Failed" ]; then
            echo "‚ùå Migration failed!"
            
            # Get failed pod logs for debugging
            failed_pods=$(kubectl get pods -n zoneapi -l job-name="$latest_job" --field-selector=status.phase=Failed -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
            for pod in $failed_pods; do
              echo "=== Failed Pod Logs: $pod ==="
              kubectl logs "$pod" -n zoneapi || echo "Cannot retrieve logs for $pod"
            done
          fi
        else
          echo "‚ö†Ô∏è  No migration job found"
        fi

    - name: üìä Post-Migration Database Verification (Fixed Environment Variables)
      timeout-minutes: 3
      run: |
        echo "=== üìä POST-MIGRATION DATABASE VERIFICATION ==="
        
        DB_HOST="${{ needs.deploy-infrastructure.outputs.postgres-host }}"
        
        if [ -z "$DB_HOST" ]; then
          echo "‚ùå Database host not available"
          exit 1
        fi
        
        echo "Database Host: $DB_HOST"
        echo "Creating post-migration verification pod..."
        
        # Create pod manifest with variables properly substituted
        cat > /tmp/post-migration-check.yaml << EOF
        apiVersion: v1
        kind: Pod
        metadata:
          name: post-migration-check
          namespace: zoneapi
        spec:
          restartPolicy: Never
          containers:
          - name: post-migration
            image: postgres:15-alpine
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: zoneapi-db-secret
                  key: password
            - name: DB_HOST
              value: "$DB_HOST"
            command:
            - /bin/sh
            - -c
            - |
              echo "Post-migration database verification on host: \$DB_HOST"
              echo ""
              echo "Current database tables:"
              psql -h "\$DB_HOST" -U postgres -d zone -c "\\dt"
              
              echo ""
              echo "Migration history summary:"
              psql -h "\$DB_HOST" -U postgres -d zone -c "SELECT COUNT(*) as total_migrations FROM __EFMigrationsHistory;"
              
              echo ""
              echo "Applied migrations:"
              psql -h "\$DB_HOST" -U postgres -d zone -c "SELECT migration_id, product_version FROM __EFMigrationsHistory ORDER BY migration_id;"
              
              echo ""
              echo "Database verification summary:"
              echo "- Connected to database: zone"
              echo "- Host: \$DB_HOST"
              echo "- Migration status: $(psql -h "\$DB_HOST" -U postgres -d zone -t -c "SELECT COUNT(*) FROM __EFMigrationsHistory;" | xargs) migrations applied"
              
              echo ""
              echo "‚úÖ Post-migration verification completed successfully"
        EOF
        
        # Apply the pod manifest
        kubectl apply -f /tmp/post-migration-check.yaml
        
        echo "Waiting for post-migration verification to complete..."
        
        # Wait for pod to complete
        timeout=120
        while [ $timeout -gt 0 ]; do
          status=$(kubectl get pod post-migration-check -n zoneapi -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
          if [ "$status" = "Succeeded" ]; then
            echo "‚úÖ Post-migration verification completed successfully"
            kubectl logs post-migration-check -n zoneapi
            break
          elif [ "$status" = "Failed" ]; then
            echo "‚ùå Post-migration verification failed"
            kubectl logs post-migration-check -n zoneapi || echo "No logs available"
            kubectl describe pod post-migration-check -n zoneapi
            exit 1
          fi
          echo "Waiting for verification to complete... (status: $status, timeout: ${timeout}s)"
          sleep 5
          timeout=$((timeout - 5))
        done
        
        if [ $timeout -le 0 ]; then
          echo "‚ùå Post-migration verification timed out"
          exit 1
        fi
        
        # Clean up pod
        kubectl delete pod post-migration-check -n zoneapi --ignore-not-found=true
        
        echo "‚úÖ Post-migration database verification completed!"

    - name: üõ†Ô∏è Comprehensive Migration Troubleshooting (Fixed Environment Variables)
      if: failure()
      run: |
        echo "=== üõ†Ô∏è COMPREHENSIVE MIGRATION TROUBLESHOOTING ==="
        
        DB_HOST="${{ needs.deploy-infrastructure.outputs.postgres-host }}"
        
        if [ -z "$DB_HOST" ]; then
          echo "‚ùå Database host not available for troubleshooting"
          DB_HOST="unknown"
        fi
        
        echo "Database Host for troubleshooting: $DB_HOST"
        
        # Set environment variables for the debug script
        export DB_HOST="$DB_HOST"
        export DB_PASSWORD="${{ secrets.POSTGRES_ADMIN_PASSWORD }}"
        export NAMESPACE="zoneapi"
        
        echo "Running comprehensive troubleshooting with proper environment variables..."
        
        # Make debug script executable
        chmod +x ./scripts/debug-migration-status.sh
        
        # Run comprehensive troubleshooting
        ./scripts/debug-migration-status.sh zoneapi || echo "Troubleshooting script completed"
        
        echo ""
        echo "Running health check diagnostics..."
        chmod +x ./scripts/debug-health-checks.sh
        ./scripts/debug-health-checks.sh || echo "Health check script completed with issues"
    
    - name: ‚úÖ Verify Migration Success (Enhanced Detection)
      if: always()
      run: |
        echo "=== FINAL MIGRATION STATUS SUMMARY ==="
        
        # Wait a moment for job status to stabilize
        sleep 5
        
        # Get all migration jobs and their statuses
        echo "Checking all migration jobs in namespace..."
        kubectl get jobs -n zoneapi -l app.kubernetes.io/component=migration -o wide || echo "No migration jobs with component label found"
        
        # Also check for jobs with migration in the name (fallback)
        echo ""
        echo "Checking jobs with 'migration' in name..."
        kubectl get jobs -n zoneapi --no-headers 2>/dev/null | grep -i migration || echo "No jobs with migration in name found"
        
        # Enhanced job detection using multiple methods
        completed_jobs=""
        failed_jobs=""
        
        # Method 1: Check by component label
        all_migration_jobs=$(kubectl get jobs -n zoneapi -l app.kubernetes.io/component=migration -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
        
        # Method 2: Check by name pattern if no labeled jobs found
        if [ -z "$all_migration_jobs" ]; then
          all_migration_jobs=$(kubectl get jobs -n zoneapi --no-headers 2>/dev/null | grep -i migration | awk '{print $1}' || echo "")
        fi
        
        echo "Found migration jobs: ${all_migration_jobs:-[NONE]}"
        
        if [ -n "$all_migration_jobs" ]; then
          for job in $all_migration_jobs; do
            echo ""
            echo "Analyzing job: $job"
            
            # Get job status using multiple methods
            job_status=$(kubectl get job "$job" -n zoneapi -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "")
            job_failed=$(kubectl get job "$job" -n zoneapi -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null || echo "")
            
            # Alternative status check
            completions=$(kubectl get job "$job" -n zoneapi -o jsonpath='{.status.completions}' 2>/dev/null || echo "0")
            succeeded=$(kubectl get job "$job" -n zoneapi -o jsonpath='{.status.succeeded}' 2>/dev/null || echo "0")
            failed_count=$(kubectl get job "$job" -n zoneapi -o jsonpath='{.status.failed}' 2>/dev/null || echo "0")
            
            echo "  Status - Complete: ${job_status}, Failed: ${job_failed}"
            echo "  Counts - Completions: ${completions}, Succeeded: ${succeeded}, Failed: ${failed_count}"
            
            # Show recent logs for debugging
            echo "  Recent logs:"
            kubectl logs -l job-name="$job" -n zoneapi --tail=5 2>/dev/null | sed 's/^/    /' || echo "    No logs available"
            
            # Determine if job is completed or failed
            if [ "$job_status" = "True" ] || [ "$succeeded" = "$completions" ] && [ "$completions" != "0" ]; then
              completed_jobs="$completed_jobs $job"
              echo "  ‚úÖ Job completed successfully"
            elif [ "$job_failed" = "True" ] || [ "$failed_count" != "0" ]; then
              failed_jobs="$failed_jobs $job"
              echo "  ‚ùå Job failed"
            else
              echo "  ‚è≥ Job still running or unknown status"
            fi
          done
          
          echo ""
          echo "Summary:"
          echo "Completed jobs:${completed_jobs:-[NONE]}"
          echo "Failed jobs:${failed_jobs:-[NONE]}"
          
          if [ -n "$completed_jobs" ]; then
            echo "‚úÖ Successfully completed migration jobs found"
            exit 0
          elif [ -n "$failed_jobs" ]; then
            echo "‚ùå Failed migration jobs found"
            exit 1
          else
            echo "‚ö†Ô∏è No clearly completed or failed jobs found - jobs may still be running"
            
            # Check if any pods are still running
            running_pods=$(kubectl get pods -n zoneapi -l app.kubernetes.io/component=migration --field-selector=status.phase=Running --no-headers 2>/dev/null | wc -l || echo "0")
            if [ "$running_pods" -gt 0 ]; then
              echo "Found $running_pods migration pods still running - this is normal"
              exit 0
            else
              echo "No running migration pods found"
          exit 1
        fi
          fi
        else
          echo "‚ö†Ô∏è No migration jobs found"
          
          # Check if there are any migration-related pods
          migration_pods=$(kubectl get pods -n zoneapi --no-headers 2>/dev/null | grep -i migration || echo "")
          if [ -n "$migration_pods" ]; then
            echo "Found migration pods without jobs:"
            echo "$migration_pods"
            
            # Check if any completed successfully
            succeeded_pods=$(echo "$migration_pods" | grep -i completed || echo "")
            if [ -n "$succeeded_pods" ]; then
              echo "‚úÖ Found completed migration pods - migration likely succeeded"
              exit 0
            fi
          fi
          
          exit 1
        fi

  # Application Deployment Job (Completely Separate)
  deploy-application:
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure, docker-build-push, run-migration]
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Setup Helm
      uses: azure/setup-helm@v3
      with:
        version: ${{ env.HELM_VERSION }}
    
    - name: Get AKS credentials
      run: |
        az aks get-credentials \
          --resource-group ${{ needs.deploy-infrastructure.outputs.resource-group }} \
          --name ${{ needs.deploy-infrastructure.outputs.aks-cluster-name }} \
          --overwrite-existing
    
    - name: üîß Validate Deployment Configuration
      run: |
        echo "=== üîß VALIDATING DEPLOYMENT CONFIGURATION ==="
        
        # Validate required variables
        ACR_LOGIN_SERVER="${{ needs.deploy-infrastructure.outputs.acr-login-server }}"
        IMAGE_TAG="${{ needs.docker-build-push.outputs.image-tag }}"
        POSTGRES_HOST="${{ needs.deploy-infrastructure.outputs.postgres-host }}"
        DB_PASSWORD="${{ secrets.POSTGRES_ADMIN_PASSWORD }}"
        
        echo "Deployment Configuration:"
        echo "- ACR Login Server: ${ACR_LOGIN_SERVER}"
        echo "- Image Tag: ${IMAGE_TAG}"
        echo "- PostgreSQL Host: ${POSTGRES_HOST}"
        echo "- Password: ${DB_PASSWORD:+[PROVIDED]}"
        
        # Validate all required values are present
        if [ -z "$ACR_LOGIN_SERVER" ] || [ -z "$IMAGE_TAG" ] || [ -z "$POSTGRES_HOST" ] || [ -z "$DB_PASSWORD" ]; then
          echo "‚ùå Missing required deployment variables"
          exit 1
        fi
        
        echo "‚úÖ All deployment variables validated successfully!"

    - name: üîß Fix Secret Ownership for Helm
      timeout-minutes: 2
      run: |
        echo "=== üîß FIXING SECRET OWNERSHIP FOR HELM ==="
        
        # Check if secret exists and has Helm metadata
        if kubectl get secret zoneapi-db-secret -n zoneapi &>/dev/null; then
          echo "Secret zoneapi-db-secret exists, checking Helm metadata..."
          
          # Check if secret has Helm labels
          HELM_MANAGED=$(kubectl get secret zoneapi-db-secret -n zoneapi -o jsonpath='{.metadata.labels.app\.kubernetes\.io/managed-by}' 2>/dev/null || echo "")
          
          if [ "$HELM_MANAGED" != "Helm" ]; then
            echo "‚ö†Ô∏è Secret not managed by Helm, fixing ownership..."
            
            # Save the password value
            DB_PASSWORD=$(kubectl get secret zoneapi-db-secret -n zoneapi -o jsonpath='{.data.password}' 2>/dev/null || echo "")
            
            if [ -n "$DB_PASSWORD" ]; then
              echo "‚úÖ Retrieved existing password"
              
              # Delete the existing secret
              kubectl delete secret zoneapi-db-secret -n zoneapi --ignore-not-found=true
              echo "üóëÔ∏è Deleted existing secret"
              
              # Create new secret with Helm metadata
              kubectl create secret generic zoneapi-db-secret \
                --namespace=zoneapi \
                --from-literal=password="${{ secrets.POSTGRES_ADMIN_PASSWORD }}" \
                --dry-run=client -o yaml | \
              kubectl label --local -f - \
                app.kubernetes.io/managed-by=Helm \
                app.kubernetes.io/instance=zoneapi \
                app.kubernetes.io/name=zoneapi -o yaml | \
              kubectl annotate --local -f - \
                meta.helm.sh/release-name=zoneapi \
                meta.helm.sh/release-namespace=zoneapi -o yaml | \
              kubectl apply -f -
              
              echo "‚úÖ Secret recreated with proper Helm metadata"
            else
              echo "‚ö†Ô∏è Could not retrieve existing password, will let Helm create new secret"
              kubectl delete secret zoneapi-db-secret -n zoneapi --ignore-not-found=true
            fi
          else
            echo "‚úÖ Secret already managed by Helm"
          fi
        else
          echo "‚ÑπÔ∏è Secret does not exist, Helm will create it"
        fi

    - name: üöÄ Deploy Application (Post-Migration)
      timeout-minutes: 5
      run: |
        echo "=== üöÄ DEPLOYING APPLICATION (POST-MIGRATION) ==="
        echo "Image: ${{ needs.deploy-infrastructure.outputs.acr-login-server }}/${{ env.IMAGE_NAME }}:${{ needs.docker-build-push.outputs.image-tag }}"
        echo "Database: ${{ needs.deploy-infrastructure.outputs.postgres-host }}"
        
        # Deploy application with proven working configuration
        helm upgrade --install zoneapi ./charts/zoneapi \
          --namespace zoneapi \
          --create-namespace \
          --set migration.enabled=false \
          --set image.repository=${{ needs.deploy-infrastructure.outputs.acr-login-server }}/${{ env.IMAGE_NAME }} \
          --set image.tag=${{ needs.docker-build-push.outputs.image-tag }} \
          --set imagePullSecrets[0].name=acr-secret \
          --set database.host=${{ needs.deploy-infrastructure.outputs.postgres-host }} \
          --set database.password=${{ secrets.POSTGRES_ADMIN_PASSWORD }} \
          --set livenessProbe.enabled=true \
          --set readinessProbe.enabled=true \
          --set replicaCount=1 \
          --force \
          --wait --timeout=5m \
          --debug
        
        echo "‚úÖ Application deployed successfully!"
    
    - name: üîç Verify Deployment & Health
      if: always()
      run: |
        echo "=== üîç DEPLOYMENT VERIFICATION & HEALTH CHECKS ==="
        
        echo "=== Pods Status ==="
        kubectl get pods -n zoneapi -o wide
        
        echo "=== Services ==="
        kubectl get services -n zoneapi -o wide
        
        echo "=== Ingress ==="
        kubectl get ingress -n zoneapi -o wide || echo "No ingress found"
        
        # Get the main application pod
        app_pod=$(kubectl get pods -n zoneapi -l app.kubernetes.io/name=zoneapi -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
        
        if [ -n "$app_pod" ]; then
          echo ""
          echo "=== Application Pod: $app_pod ==="
          
          # Check pod status
          pod_status=$(kubectl get pod "$app_pod" -n zoneapi -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
          ready_status=$(kubectl get pod "$app_pod" -n zoneapi -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "Unknown")
          
          echo "Pod Status: $pod_status"
          echo "Ready Status: $ready_status"
          
          # Verify environment variables are correctly set
          echo ""
          echo "=== Environment Variables Verification ==="
          kubectl exec -n zoneapi "$app_pod" -- env | grep -E "(ConnectionStrings|DB_)" | sort || echo "Could not retrieve environment variables"
          
          # Test health endpoint
          echo ""
          echo "=== Health Endpoint Test ==="
          if kubectl exec -n zoneapi "$app_pod" -- curl -f --max-time 10 http://localhost:8080/health 2>/dev/null; then
            echo ""
            echo "‚úÖ Health check passed!"
          else
            echo "‚ùå Health check failed!"
            
            echo ""
            echo "=== Application Logs for Debugging ==="
            kubectl logs "$app_pod" -n zoneapi --tail=30 || echo "Could not retrieve logs"
          fi
        else
          echo "‚ö†Ô∏è No application pod found"
        fi
        
        echo ""
        echo "=== Recent Events ==="
        kubectl get events -n zoneapi --sort-by='.lastTimestamp' | tail -20
    
    - name: üèÅ Final Health & API Validation
      run: |
        echo "=== üèÅ FINAL HEALTH & API VALIDATION ==="
        
        # Get pod name
        pod_name=$(kubectl get pods -n zoneapi -l app.kubernetes.io/name=zoneapi -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
        
        if [ -n "$pod_name" ]; then
          echo "Testing pod: $pod_name"
          
          # Wait for pod to be ready with timeout
          echo "Waiting for pod to be ready..."
          if kubectl wait --for=condition=ready pod/$pod_name -n zoneapi --timeout=120s; then
            echo "‚úÖ Pod is ready!"
            
            # Test health endpoint with detailed response
            echo ""
            echo "Testing health endpoint..."
            if health_response=$(kubectl exec -n zoneapi "$pod_name" -- curl -f --max-time 15 http://localhost:8080/health 2>/dev/null); then
              echo "‚úÖ Health endpoint responded successfully!"
              echo "Health Response: $health_response"
              
              # Parse the health response to verify database connectivity
              if echo "$health_response" | grep -q '"connected":true'; then
                echo "‚úÖ Database connectivity confirmed!"
              else
                echo "‚ö†Ô∏è Database connectivity issue detected in health response"
              fi
              
              # Test a simple API endpoint
              echo ""
              echo "Testing Doctors API endpoint..."
              if kubectl exec -n zoneapi "$pod_name" -- curl -f --max-time 10 http://localhost:8080/api/doctors 2>/dev/null; then
                echo ""
                echo "‚úÖ API endpoints are accessible!"
              else
                echo "‚ö†Ô∏è API endpoint test failed (may be expected if no data)"
              fi
              
            else
              echo "‚ùå Health endpoint test failed!"
              echo "Checking pod logs for issues..."
              kubectl logs "$pod_name" -n zoneapi --tail=20
              exit 1
            fi
          else
            echo "‚ùå Pod failed to become ready within timeout"
            kubectl describe pod "$pod_name" -n zoneapi
            exit 1
          fi
        else
          echo "‚ùå No pods found to test"
          exit 1
        fi
        
        echo ""
        echo "üéâ All deployment validation tests passed!"
        
        # Show external endpoint
        external_ip=$(kubectl get service zoneapi -n zoneapi -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "pending")
        if [ "$external_ip" != "pending" ] && [ -n "$external_ip" ]; then
          echo ""
          echo "üåê Application is accessible at:"
          echo "   Health Check: http://$external_ip:8080/health"
          echo "   Doctors API:  http://$external_ip:8080/api/doctors"
          echo "   Patients API: http://$external_ip:8080/api/patients"
        else
          echo ""
          echo "‚è≥ LoadBalancer IP is still provisioning..."
        fi
    
    - name: üß™ Run Comprehensive Deployment Validation
      run: |
        echo "=== üß™ COMPREHENSIVE DEPLOYMENT VALIDATION ==="
        
        # Make script executable and run comprehensive validation
        chmod +x ./scripts/validate-deployment.sh
        
        if ./scripts/validate-deployment.sh; then
          echo ""
          echo "‚úÖ Comprehensive validation completed successfully!"
        else
          echo ""
          echo "‚ùå Comprehensive validation failed!"
          exit 1
        fi 